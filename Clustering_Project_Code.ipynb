{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35roXDEMudbw"
   },
   "source": [
    "# GUC Clustering Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCwbCzREudb1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIiItKbYudb2"
   },
   "source": [
    "**Objective:** \n",
    "The objective of this project teach students how to apply clustering to real data sets\n",
    "\n",
    "The projects aims to teach student: \n",
    "* Which clustering approach to use\n",
    "* Compare between Kmeans, Hierarchal, DBScan, and Gaussian Mixtures  \n",
    "* How to tune the parameters of each data approach\n",
    "* What is the effect of different distance functions (optional) \n",
    "* How to evaluate clustering approachs \n",
    "* How to display the output\n",
    "* What is the effect of normalizing the data \n",
    "\n",
    "Students in this project will use ready-made functions from Sklearn, plotnine, numpy and pandas \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MtHElDYdudb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotnine in d:\\anaconda\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in d:\\anaconda\\lib\\site-packages (from plotnine) (3.8.3)\n",
      "Requirement already satisfied: pandas>=2.1.0 in d:\\anaconda\\lib\\site-packages (from plotnine) (2.2.1)\n",
      "Requirement already satisfied: mizani==0.11.0 in d:\\anaconda\\lib\\site-packages (from plotnine) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.23.0 in d:\\anaconda\\lib\\site-packages (from plotnine) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in d:\\anaconda\\lib\\site-packages (from plotnine) (1.12.0)\n",
      "Requirement already satisfied: statsmodels>=0.14.0 in d:\\anaconda\\lib\\site-packages (from plotnine) (0.14.1)\n",
      "Requirement already satisfied: tzdata in d:\\anaconda\\lib\\site-packages (from mizani==0.11.0->plotnine) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.6.0->plotnine) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.6.0->plotnine) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.6.0->plotnine) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.6.0->plotnine) (1.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.6.0->plotnine) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.6.0->plotnine) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.6.0->plotnine) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.6.0->plotnine) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in d:\\anaconda\\lib\\site-packages (from matplotlib>=3.6.0->plotnine) (6.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas>=2.1.0->plotnine) (2021.3)\n",
      "Requirement already satisfied: patsy>=0.5.4 in d:\\anaconda\\lib\\site-packages (from statsmodels>=0.14.0->plotnine) (0.5.6)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.6.0->plotnine) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\anaconda\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.6.0->plotnine) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# if plotnine is not installed in Jupter then use the following command to install it \n",
    "!pip install plotnine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RHS5ZoQudb4"
   },
   "source": [
    "Running this project require the following imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QrueqJenudb5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "from sklearn.datasets import make_blobs\n",
    "from plotnine import *   \n",
    "# StandardScaler is a function to normalize the data \n",
    "# You may also check MinMaxScaler and MaxAbsScaler \n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster_Distance: numpy array of shape (num_points, K), containing distances between each data point and each cluster centroid.\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2)**2))\n",
    "\n",
    "def pearson_correlation_distance(data_array, centroid,mean_points,mean_centroid,new_points):    \n",
    "    new_centroid = centroid - mean_centroid\n",
    "    num = np.sum(new_points * new_centroid, axis=1)\n",
    "    den = (np.sqrt(np.sum(new_points ** 2, axis=1))) * (np.sqrt(np.sum(new_centroid ** 2)))\n",
    "    return 1 - (num / den)\n",
    "\n",
    "def GUC_Distance ( Cluster_Centroids, Data_points, Distance_Type ):\n",
    "    data_array = Data_points.to_numpy()  # Convert DataFrame to numpy array\n",
    "    num_clusters = Cluster_Centroids.shape[0]\n",
    "    num_data_points = Data_points.shape[0]\n",
    "    mean_points = np.mean(Data_points, axis=0)\n",
    "    mean_centroid = np.mean(Cluster_Centroids, axis=0)\n",
    "    new_points = Data_points - mean_points\n",
    "    distances = np.zeros((num_data_points, num_clusters))\n",
    "    for i in range(num_clusters):\n",
    "        centroid = Cluster_Centroids.iloc[i, :].values\n",
    "        if Distance_Type == 'euclidean':\n",
    "            distances[:, i] = np.apply_along_axis(lambda x: euclidean_distance(x, centroid), axis=1, arr=data_array)\n",
    "        elif Distance_Type == 'pearson':\n",
    "            distances[:, i] = pearson_correlation_distance(data_array, centroid,mean_points,mean_centroid,new_points)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster(X, km, num_clusters):\n",
    "    color = ['b', 'r', 'g', 'c', 'm', 'y', 'k', 'Turquoise', 'LimeGreen', 'pink']  # List colors\n",
    "    alpha = 0.5  # color opaque\n",
    "    s = 100\n",
    "\n",
    "    if num_clusters == 0 or not km:\n",
    "        plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=color[0], alpha=alpha, s=s)\n",
    "    else:\n",
    "        num_columns = X.shape[1]\n",
    "        fig, ax = plt.subplots(num_columns, num_columns, figsize=(30, 30))\n",
    "        for i in range(min(num_clusters, len(km))):\n",
    "            cluster_assignments, cluster_centers = km[i]\n",
    "            arr = X.to_numpy() \n",
    "            for m in range(num_columns):\n",
    "                for n in range(num_columns):\n",
    "                    indices = arr[cluster_assignments == i]\n",
    "                    ax[m, n].scatter(arr[cluster_assignments == i, m], arr[cluster_assignments == i, n], c=color[i], alpha=alpha, marker='o', s=s)\n",
    "                    ax[m, n].scatter(cluster_centers.iloc[i, m], cluster_centers.iloc[i, n], c=color[i], marker='x', s=200)\n",
    "                   \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GUC_Kmean ( Data_points, Number_of_Clusters,  Distance_Type ):\n",
    "    tolerance=0.001\n",
    "    km=[]\n",
    "       # Step 1: Initialize cluster heads\n",
    "    #cluster_heads = initialize_cluster_heads(Data_points, Number_of_Clusters)\n",
    "    data_ranges = np.ptp(Data_points.values, axis=0)\n",
    "    cluster_heads_df = np.random.rand(Number_of_Clusters, Data_points.shape[1]) * data_ranges\n",
    "    cluster_heads = pd.DataFrame(cluster_heads_df, columns=Data_points.columns)\n",
    "    \n",
    "    \n",
    "    previous_distortion_function = float('inf')\n",
    "    thres_prev = np.inf\n",
    "    Cluster_Metric=0\n",
    "    mean_square_distances = np.zeros(Number_of_Clusters)\n",
    "    while True:\n",
    "        # Step 2: Cluster Assignment\n",
    "        Final_Cluster_Distance = GUC_Distance(cluster_heads, Data_points, Distance_Type)\n",
    "        cluster_assignments = np.argmin(Final_Cluster_Distance, axis=1)\n",
    "        thres = 0\n",
    "        mean_square_distances = np.zeros(Number_of_Clusters)\n",
    "       # Step 3: Calculate Mean Square Distance for each cluster\n",
    "        for i in range(Number_of_Clusters):\n",
    "            cluster_points = Data_points.iloc[cluster_assignments == i]\n",
    "            squared_distances = np.sum((cluster_points - cluster_heads.iloc[i, :]) ** 2, axis=1)\n",
    "            mean_square_distances[i] = np.nanmean(squared_distances)  # Calculate mean while ignoring NaN values\n",
    "            mean_square_distances = np.nan_to_num(mean_square_distances, nan=0.0)  # Replace NaN values with zero\n",
    "            thres += np.sum((cluster_points - cluster_heads.iloc[i, :])**2)\n",
    "\n",
    "        #print(\"mean_square_distances\",mean_square_distances)\n",
    "        \n",
    "\n",
    "        # Step 4: Stopping condition : # if the sum of mean square distances are close or smaller than the tolarnce\n",
    "        if np.sum(mean_square_distances) < tolerance or np.allclose(mean_square_distances, previous_distortion_function):\n",
    "            break\n",
    "        eps = 0.0001\n",
    "        if (abs(thres - thres_prev) < eps).any():\n",
    "            break\n",
    "        thres_prev = thres\n",
    "\n",
    "        previous_distortion_function=mean_square_distances.copy()\n",
    "        \n",
    "        #print(\"Sum_of_mean_square_dis\",sum(mean_square_distances))   \n",
    "        \n",
    "        \n",
    "          # Step 3: Update centroids\n",
    "        for i in range(Number_of_Clusters):\n",
    "            cluster_points = Data_points.iloc[cluster_assignments == i]  # Use iloc here to select rows\n",
    "            if len(cluster_points) > 0:\n",
    "                cluster_heads.iloc[i, :] = np.mean(cluster_points, axis=0)\n",
    "        \n",
    "        \n",
    "    # Step 6: Calculate Cluster Metric (Distortion Function)\n",
    "        km.append((cluster_assignments,cluster_heads))\n",
    "    \n",
    "    #Cluster_Metric = sum(mean_square_distances)\n",
    "    Cluster_Metric += sum(thres)\n",
    "    #print(\"Clusrer_Metric\",Cluster_Metric)\n",
    "    display_cluster(Data_points,km,Number_of_Clusters)\n",
    "    return [ Final_Cluster_Distance , Cluster_Metric ]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_Examples(example_data,distance_type):\n",
    "    \n",
    "    #example_data = pd.DataFrame(example_data, columns=['X', 'Y'])\n",
    "    Num_of_clusters=[]\n",
    "    cluster_Performance_mertic=[]\n",
    "    # Check if example_data is already a DataFrame\n",
    "    if not isinstance(example_data, pd.DataFrame):\n",
    "        # Convert example_data to DataFrame with columns 'X' and 'Y'\n",
    "        example_data = pd.DataFrame(example_data, columns=['X', 'Y'])\n",
    "    for i in range(2,11):\n",
    "        clusters=i\n",
    "        print(\"Graphs_For_Cluster=\",i)\n",
    "        [Final_Cluster_Distance , Cluster_Metric ] = GUC_Kmean(example_data,i,distance_type)\n",
    "        cluster_Performance_mertic.append(Cluster_Metric)\n",
    "        Num_of_clusters.append(clusters)\n",
    "        print(\"cluster_Performance\",cluster_Performance_mertic)\n",
    "        \n",
    "    plt.plot(Num_of_clusters,cluster_Performance_mertic, marker='o',linestyle='-')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Performance Metric')\n",
    "    plt.title('Cluster Performance Metric vs Number of Clusters')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_silhouette_scoreeeeeeee(example_data,distance_type):\n",
    "    cluster_metrics_e = []\n",
    "    num_clusters_list_e = []\n",
    "    silhouette_scores = []\n",
    "    # Check if example_data is already a DataFrame\n",
    "    if not isinstance(example_data, pd.DataFrame):\n",
    "        # Convert example_data to DataFrame with columns 'X' and 'Y'\n",
    "        example_data = pd.DataFrame(example_data, columns=['X', 'Y'])\n",
    "        \n",
    "    k_values= range(2,11)\n",
    "    for i in k_values:\n",
    "        Number_of_Clusters = i\n",
    "        print(\"Graphs_For_Cluster=\",i)\n",
    "        [Final_Cluster_Distance, Cluster_Metric]=GUC_Kmean(example_data,Number_of_Clusters,distance_type)\n",
    "        cluster_metrics_e.append(Cluster_Metric)\n",
    "        num_clusters_list_e.append(Number_of_Clusters)\n",
    "\n",
    "        # Check number of unique labels\n",
    "        cluster_labels = Final_Cluster_Distance.argmin(axis=1)\n",
    "        unique_labels = len(set(cluster_labels))\n",
    "    \n",
    "    # Calculate silhouette score if more than one unique label\n",
    "        if unique_labels > 1:\n",
    "            sil_score = silhouette_score(example_data, cluster_labels)\n",
    "            silhouette_scores.append(sil_score)\n",
    "        else:\n",
    "            silhouette_scores.append(None)  # Placeholder if silhouette score cannot be calculated\n",
    "        \n",
    "        \n",
    "    # Plot silhouette scores versus K\n",
    "    silhouette_scores_filteredpn = [score for score in silhouette_scores if score is not None]\n",
    "    num_clusters_filteredpn = [num_clusters_list_e[i] for i, score in enumerate(silhouette_scores) if score is not None]\n",
    "    best_k_idxpn = np.argmax(silhouette_scores_filteredpn)\n",
    "    best_k_pin = num_clusters_filteredpn[best_k_idxpn]\n",
    "    print(\"Best K using silhouette score:\", best_k_pin)\n",
    "    plt.plot(num_clusters_filteredpn, silhouette_scores_filteredpn, marker='o')\n",
    "    plt.xlabel('Number of clusters (K)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score vs Number of Clusters')\n",
    "    plt.xticks(num_clusters_filteredpn)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZnIbT3Mudb6"
   },
   "source": [
    "## Multi Blob Data Set \n",
    "* The Data Set generated below has 6 cluster with varying number of users and varing densities\n",
    "* Cluster the data set below using \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "JeSqG318udb7",
    "outputId": "078fad92-3073-4558-b1e8-f0acd8d85d34"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8,8]\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "n_bins = 6  \n",
    "centers = [(-3, -3), (0, 0), (5,2.5),(-1, 4), (4, 6), (9,7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100,150, 300, 400,300, 200], n_features=2, cluster_std=[1.3,0.6, 1.2, 1.7,0.9,1.7],\n",
    "                  centers=centers, shuffle=False, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDSIGjubudb8"
   },
   "source": [
    "### Kmeans \n",
    "* Use Kmeans with different values of K to cluster the above data \n",
    "* Display the outcome of each value of K \n",
    "* Plot distortion function versus K and choose the approriate value of k \n",
    "* Plot the silhouette_score versus K and use it to choose the best K \n",
    "* Store the silhouette_score for the best K for later comparison with other clustering techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def kmeans_clustering(example_data, k_values):\n",
    "    # Initialize lists to store SSE and silhouette scores for different K values\n",
    "    sse_values = []\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for k in k_values:\n",
    "        # Perform K-means clustering\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        cluster_labels = kmeans.fit_predict(example_data)\n",
    "        \n",
    "        # Calculate Sum of Squared Errors (SSE) for the clustering\n",
    "        sse = kmeans.inertia_\n",
    "        sse_values.append(sse)\n",
    "        \n",
    "        # Calculate silhouette score for the clustering\n",
    "        silhouette_avg = silhouette_score(example_data, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "        # Plot clusters (for visualization, you can customize this part)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(example_data[:, 0], example_data[:, 1], c=cluster_labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "        plt.title(f'K-means Clustering with K={k}')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.show()\n",
    "\n",
    "    # Find the index of the maximum silhouette score\n",
    "    best_k_idx = np.argmax(silhouette_scores)\n",
    "    # Get the corresponding value of K\n",
    "    best_k = k_values[best_k_idx]  \n",
    "    \n",
    "    # Plot SSE versus K\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, sse_values, marker='o', linestyle='-')\n",
    "    plt.title('Sum of Squared Errors (SSE) versus Number of Clusters (K)')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('SSE')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Best K using silhouette score:\", best_k)\n",
    "    # Plot silhouette score versus K\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, silhouette_scores, marker='o', linestyle='-')\n",
    "    plt.title('Silhouette Score versus Number of Clusters (K)')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE7dvpOAudb9"
   },
   "source": [
    "### Hierarchal Clustering\n",
    "* Use AgglomerativeClustering function to  to cluster the above data \n",
    "* In the  AgglomerativeClustering change the following parameters \n",
    "    * Affinity (use euclidean, manhattan and cosine)\n",
    "    * Linkage( use average and single )\n",
    "    * Distance_threshold (try different)\n",
    "* For each of these trials plot the Dendograph , calculate the silhouette_score and display the resulting clusters  \n",
    "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
    "* Record your observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Importing plt from matplotlib.pyplot\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import seaborn as sns\n",
    "\n",
    "def Hierarchal_clustering(example_data):\n",
    "    # Define parameter combinations to try\n",
    "    affinities = ['euclidean', 'manhattan', 'cosine']\n",
    "    linkages = ['average', 'single']\n",
    "    distance_thresholds = [None, 2, 5]  # Adjust threshold values as needed\n",
    "\n",
    "    # Initialize variables to store the best silhouette score and its corresponding parameters\n",
    "    best_silhouette_score = -1  # Initialize with a value that ensures any calculated silhouette score will be better\n",
    "    best_params = None\n",
    "    # Loop through parameter combinations\n",
    "    for affinity in affinities:\n",
    "        if affinity == 'manhattan':\n",
    "            affinity = 'cityblock'\n",
    "        for linkage_type in linkages:\n",
    "            for distance_threshold in distance_thresholds:\n",
    "                # Perform hierarchical clustering\n",
    "                if distance_threshold is not None:\n",
    "                    Z = linkage(example_data, metric=affinity, method=linkage_type)\n",
    "                    cluster_labels = fcluster(Z, t=distance_threshold, criterion='distance')\n",
    "                    plt.figure(figsize=(20, 18))\n",
    "                    plt.title(f\"Dendrogram ({affinity}, {linkage_type}, Threshold={distance_threshold})\")\n",
    "                else:\n",
    "                    clustering = AgglomerativeClustering(n_clusters=2, affinity=affinity, linkage=linkage_type)\n",
    "                    cluster_labels = clustering.fit_predict(example_data)\n",
    "                    Z = linkage(example_data, metric=affinity, method=linkage_type)\n",
    "                    plt.figure(figsize=(20, 18))\n",
    "                    plt.title(f\"Dendrogram ({affinity}, {linkage_type}, n_clusters=2)\")\n",
    "                dendrogram(Z)     \n",
    "                # Plot dendrogram       \n",
    "                plt.xlabel('Data points')\n",
    "                plt.ylabel('Distance')\n",
    "                plt.show()\n",
    "                # Plot resulting clusters\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.scatter(example_data[:, 0], example_data[:, 1], c=cluster_labels, cmap='rainbow', marker='o', edgecolors='k')\n",
    "                plt.title(f\"Clusters - Affinity: {affinity}, Linkage: {linkage_type}, Distance Threshold: {distance_threshold}\")\n",
    "                plt.xlabel('Feature 1')\n",
    "                plt.ylabel('Feature 2')\n",
    "                plt.colorbar(label='Cluster')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "\n",
    "                # Calculate silhouette score\n",
    "                # Check if multiple clusters are formed\n",
    "                if len(np.unique(cluster_labels)) > 1:\n",
    "                    silhouette_avg = silhouette_score(pd.DataFrame(example_data), cluster_labels)\n",
    "                    print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "                 # Check if this silhouette score is better than the current best\n",
    "                    if silhouette_avg > best_silhouette_score:\n",
    "                        best_silhouette_score = silhouette_avg\n",
    "                        best_params = {'Affinity': affinity, 'Linkage': linkage_type, 'Distance Threshold': distance_threshold}    \n",
    "\n",
    "\n",
    "\n",
    "    # Print the best silhouette score and its corresponding parameters\n",
    "    print(\"Best Silhouette Score:\", best_silhouette_score)\n",
    "    print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3O_6WwKoudb-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myJE7vQKudb-"
   },
   "source": [
    "### DBScan\n",
    "* Use DBScan function to  to cluster the above data \n",
    "* In the  DBscan change the following parameters \n",
    "    * EPS (from 0.1 to 3)\n",
    "    * Min_samples (from 5 to 25)\n",
    "* Plot the silhouette_score versus the variation in the EPS and the min_samples\n",
    "* Plot the resulting Clusters in this case \n",
    "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
    "* Record your observations and comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def DBSCan(example_data):\n",
    "    # Define parameter ranges\n",
    "    eps_values = np.linspace(0.1, 3, num=10)  # 10 values between 0.1 and 3\n",
    "    min_samples_values = range(5, 26)  # Min_samples from 5 to 25\n",
    "\n",
    "    # Initialize variables to store the best silhouette score and its corresponding parameters\n",
    "    best_silhouette_score = -1\n",
    "    best_params = {}\n",
    "\n",
    "    # Initialize lists to store silhouette scores and corresponding parameters\n",
    "    silhouette_scores = []\n",
    "    parameters = []\n",
    "\n",
    "    # Loop through parameter combinations\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            # Perform clustering\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            cluster_labels = dbscan.fit_predict(example_data)\n",
    "\n",
    "            # Check if only one unique label is detected\n",
    "            if len(np.unique(cluster_labels)) <= 1:\n",
    "                continue\n",
    "\n",
    "            # Calculate silhouette score\n",
    "            silhouette_avg = silhouette_score(example_data, cluster_labels)\n",
    "\n",
    "            # Store silhouette score and parameters\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "            parameters.append({'EPS': eps, 'Min Samples': min_samples})\n",
    "\n",
    "            # Check if this silhouette score is better than the current best\n",
    "            if silhouette_avg > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette_avg\n",
    "                best_params = {'EPS': eps, 'Min Samples': min_samples}\n",
    "                best_cluster_labels = cluster_labels\n",
    "\n",
    "            # Plot clusters\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(example_data[:, 0], example_data[:, 1], c=cluster_labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "            plt.title(f'DBSCAN Clustering - EPS: {eps}, Min Samples: {min_samples}')\n",
    "            plt.xlabel('Feature 1')\n",
    "            plt.ylabel('Feature 2')\n",
    "            plt.colorbar(label='Cluster')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    # Plot silhouette score versus parameter variation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(silhouette_scores)), silhouette_scores, marker='o', linestyle='-')\n",
    "    plt.title('Silhouette Score versus Parameter Variation')\n",
    "    plt.xlabel('Parameter Combination')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.xticks(range(len(silhouette_scores)), parameters, rotation=90)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the best silhouette score and its corresponding parameters\n",
    "    print(\"Best Silhouette Score:\", best_silhouette_score)\n",
    "    print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip16g1QFudb_"
   },
   "source": [
    "### Gaussian Mixture\n",
    " * Use GaussianMixture function to cluster the above data \n",
    " * In GMM change the covariance_type and check the difference in the resulting proabability fit \n",
    " * Use a 2D contour plot to plot the resulting distribution (the components of the GMM) as well as the total Gaussian mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def GMMClustering(example_data):\n",
    "    # Define covariance types to test\n",
    "    covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
    "\n",
    "    # Initialize variables to store the best silhouette score and its corresponding parameters\n",
    "    best_silhouette_score = -1\n",
    "    best_params = {}\n",
    "\n",
    "    # Initialize lists to store silhouette scores and corresponding parameters\n",
    "    silhouette_scores = []\n",
    "    parameters = []\n",
    "\n",
    "    # Loop through covariance types\n",
    "    for covariance_type in covariance_types:\n",
    "        # Perform clustering\n",
    "        gmm = GaussianMixture(n_components=3, covariance_type=covariance_type)\n",
    "        cluster_labels = gmm.fit_predict(example_data)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(example_data, cluster_labels)\n",
    "\n",
    "        # Store silhouette score and parameters\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        parameters.append({'Covariance Type': covariance_type})\n",
    "        print(\"Silhouette_Score\",silhouette_scores)\n",
    "        \n",
    "        # Check if this silhouette score is better than the current best\n",
    "        if silhouette_avg > best_silhouette_score:\n",
    "            best_silhouette_score = silhouette_avg\n",
    "            best_params = {'Covariance Type': covariance_type}\n",
    "            best_cluster_labels = cluster_labels\n",
    "            \n",
    "\n",
    "        # Plot clusters and contour plot\n",
    "        plt.figure(figsize=(20, 15))\n",
    "\n",
    "        # Scatter plot for clusters\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(example_data[:, 0], example_data[:, 1], c=cluster_labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "        plt.title(f'GMM Clustering - Covariance Type: {covariance_type}')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "\n",
    "        # Contour plot for Gaussian mixture\n",
    "        plt.subplot(1, 2, 2)\n",
    "        x, y = np.meshgrid(np.linspace(example_data[:, 0].min(), example_data[:, 0].max(), 100),\n",
    "                           np.linspace(example_data[:, 1].min(), example_data[:, 1].max(), 100))\n",
    "        xy = np.column_stack([x.ravel(), y.ravel()])\n",
    "        z = -gmm.score_samples(xy)\n",
    "        z = z.reshape(x.shape)\n",
    "\n",
    "        plt.contour(x, y, z, norm=LogNorm(vmin=1.0, vmax=1000.0), levels=np.logspace(0, 3, 10))\n",
    "        plt.scatter(example_data[:, 0], example_data[:, 1], c=cluster_labels, cmap='viridis', marker='o', edgecolors='k')\n",
    "        plt.title('Gaussian Mixture Components')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Plot silhouette score versus covariance type\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(silhouette_scores)), silhouette_scores, tick_label=[param['Covariance Type'] for param in parameters])\n",
    "    plt.title('Silhouette Score versus Covariance Type')\n",
    "    plt.xlabel('Covariance Type')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Print the best silhouette score and its corresponding parameters\n",
    "    print(\"Best Silhouette Score:\", best_silhouette_score)\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m92lZkkyudb_"
   },
   "source": [
    "## iris data set \n",
    "The iris data set is test data set that is part of the Sklearn module \n",
    "which contains 150 records each with 4 features. All the features are represented by real numbers \n",
    "\n",
    "The data represents three classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "_QaCWyyCudcA",
    "outputId": "79c14dba-80cf-4d96-e69d-70763b789faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irissss      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                  5.1               3.5                1.4               0.2\n",
      "1                  4.9               3.0                1.4               0.2\n",
      "2                  4.7               3.2                1.3               0.2\n",
      "3                  4.6               3.1                1.5               0.2\n",
      "4                  5.0               3.6                1.4               0.2\n",
      "..                 ...               ...                ...               ...\n",
      "145                6.7               3.0                5.2               2.3\n",
      "146                6.3               2.5                5.0               1.9\n",
      "147                6.5               3.0                5.2               2.0\n",
      "148                6.2               3.4                5.4               2.3\n",
      "149                5.9               3.0                5.1               1.8\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "iris_data.target[[10, 25, 50]]\n",
    "#array([0, 0, 1])\n",
    "list(iris_data.target_names)\n",
    "['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "# Print the entire dataset (features and target labels)\n",
    "iris_df = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names)\n",
    "print(\"Irissss\",iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyoCVfyMudcA"
   },
   "source": [
    "* Repeat all the above clustering approaches and steps on the above data \n",
    "* Normalize the data then repeat all the above steps \n",
    "* Compare between the different clustering approaches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def kmeans_clustering_Iris(example_data, k_values):\n",
    "    # Initialize lists to store SSE and silhouette scores for different K values\n",
    "    sse_values = []\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for k in k_values:\n",
    "        # Perform K-means clustering\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        cluster_labels = kmeans.fit_predict(example_data)\n",
    "        \n",
    "        # Calculate Sum of Squared Errors (SSE) for the clustering\n",
    "        sse = kmeans.inertia_\n",
    "        sse_values.append(sse)\n",
    "        \n",
    "        # Calculate silhouette score for the clustering\n",
    "        silhouette_avg = silhouette_score(example_data, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "        # Plot clusters (for visualization, you can customize this part)\n",
    "        # Plot resulting clusters\n",
    "        plt.figure(figsize=(30, 25))\n",
    "        num_features = example_data.shape[1]\n",
    "        fig, axs = plt.subplots(num_features, num_features, figsize=(30, 25))\n",
    "        print(f'K-means Clustering with K={k}')\n",
    "        for i in range(num_features):\n",
    "            for j in range(num_features):\n",
    "                if i != j:\n",
    "                    axs[i, j].scatter(example_data.iloc[:, i], example_data.iloc[:, j], c=cluster_labels, cmap='rainbow', marker='o', edgecolors='k')\n",
    "                    axs[i, j].set_xlabel(example_data.columns[i])\n",
    "                    axs[i, j].set_ylabel(example_data.columns[j])\n",
    "                else:\n",
    "                    axs[i, j].axis('off')  # Turn off plot for same feature pairs\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Find the index of the maximum silhouette score\n",
    "    best_k_idx = np.argmax(silhouette_scores)\n",
    "    # Get the corresponding value of K\n",
    "    best_k = k_values[best_k_idx]  \n",
    "    \n",
    "    # Plot SSE versus K\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, sse_values, marker='o', linestyle='-')\n",
    "    plt.title('Sum of Squared Errors (SSE) versus Number of Clusters (K)')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('SSE')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Best K using silhouette score:\", best_k)\n",
    "    # Plot silhouette score versus K\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, silhouette_scores, marker='o', linestyle='-')\n",
    "    plt.title('Silhouette Score versus Number of Clusters (K)')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hierorichal_Irias\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Importing plt from matplotlib.pyplot\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import seaborn as sns\n",
    "\n",
    "def Hierarchal_clustering_Iris(example_data):\n",
    "    # Define parameter combinations to try\n",
    "    affinities = ['euclidean', 'manhattan', 'cosine']\n",
    "    linkages = ['average', 'single']\n",
    "    distance_thresholds = [None, 2, 5]  # Adjust threshold values as needed\n",
    "\n",
    "    # Initialize variables to store the best silhouette score and its corresponding parameters\n",
    "    best_silhouette_score = -1  # Initialize with a value that ensures any calculated silhouette score will be better\n",
    "    best_params = None\n",
    "    # Loop through parameter combinations\n",
    "    for affinity in affinities:\n",
    "        if affinity == 'manhattan':\n",
    "            affinity = 'cityblock'\n",
    "        for linkage_type in linkages:\n",
    "            for distance_threshold in distance_thresholds:\n",
    "                # Perform hierarchical clustering\n",
    "                if distance_threshold is not None:\n",
    "                    Z = linkage(example_data, metric=affinity, method=linkage_type)\n",
    "                    cluster_labels = fcluster(Z, t=distance_threshold, criterion='distance')\n",
    "                    plt.figure(figsize=(20, 18))\n",
    "                    plt.title(f\"Dendrogram ({affinity}, {linkage_type}, Threshold={distance_threshold})\")\n",
    "                else:\n",
    "                    clustering = AgglomerativeClustering(n_clusters=2, affinity=affinity, linkage=linkage_type)\n",
    "                    cluster_labels = clustering.fit_predict(example_data)\n",
    "                    Z = linkage(example_data, metric=affinity, method=linkage_type)\n",
    "                    plt.figure(figsize=(20, 18))\n",
    "                    plt.title(f\"Dendrogram ({affinity}, {linkage_type}, n_clusters=2)\")\n",
    "                dendrogram(Z,leaf_rotation=90., leaf_font_size=8.)     \n",
    "                # Plot dendrogram       \n",
    "                plt.xlabel('Data points')\n",
    "                plt.ylabel('Distance')\n",
    "                plt.show()\n",
    "                # Plot resulting clusters\n",
    "                plt.figure(figsize=(30, 25))\n",
    "                num_features = example_data.shape[1]\n",
    "                fig, axs = plt.subplots(num_features, num_features, figsize=(30, 25))\n",
    "                print(f\"Clusters - Affinity: {affinity}, Linkage: {linkage_type}, Distance Threshold: {distance_threshold}\")\n",
    "                for i in range(num_features):\n",
    "                    for j in range(num_features):\n",
    "                        if i != j:\n",
    "                            axs[i, j].scatter(example_data.iloc[:, i], example_data.iloc[:, j], c=cluster_labels, cmap='rainbow', marker='o', edgecolors='k')\n",
    "                            axs[i, j].set_xlabel(example_data.columns[i])\n",
    "                            axs[i, j].set_ylabel(example_data.columns[j])\n",
    "                        else:\n",
    "                            axs[i, j].axis('off')  # Turn off plot for same feature pairs\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                # Calculate silhouette score\n",
    "                # Check if multiple clusters are formed\n",
    "                if len(np.unique(cluster_labels)) > 1:\n",
    "                    silhouette_avg = silhouette_score(pd.DataFrame(example_data), cluster_labels)\n",
    "                    print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "                 # Check if this silhouette score is better than the current best\n",
    "                    if silhouette_avg > best_silhouette_score:\n",
    "                        best_silhouette_score = silhouette_avg\n",
    "                        best_params = {'Affinity': affinity, 'Linkage': linkage_type, 'Distance Threshold': distance_threshold}    \n",
    "\n",
    "\n",
    "\n",
    "    # Print the best silhouette score and its corresponding parameters\n",
    "    print(\"Best Silhouette Score:\", best_silhouette_score)\n",
    "    print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The silhouette score of K_means is higher than Hierarchal which means that in this dataset the K-means is better in clustering\n",
    "#of the iris dataset with K=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def DBSCan_iris(example_data):\n",
    "    # Define parameter ranges\n",
    "    eps_values = np.linspace(0.1, 3, num=10)  # 10 values between 0.1 and 3\n",
    "    min_samples_values = range(5, 26)  # Min_samples from 5 to 25\n",
    "    #eps_values = np.linspace(0.5, 3, num=10)  # 10 values between 0.1 and 3\n",
    "    #min_samples_values = range(5, 26)  # Min_samples from 5 to 25\n",
    "\n",
    "    # Initialize variables to store the best silhouette score and its corresponding parameters\n",
    "    best_silhouette_score = -1\n",
    "    best_params = {}\n",
    "\n",
    "    # Initialize lists to store silhouette scores and corresponding parameters\n",
    "    silhouette_scores = []\n",
    "    parameters = []\n",
    "\n",
    "    # Loop through parameter combinations\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            # Perform clustering\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            #print(\"DSCANNNNNNN\",dbscan)\n",
    "            cluster_labels = dbscan.fit_predict(example_data)\n",
    "            #print(\"Cluster_label\",cluster_labels)    \n",
    "            # Check if only one unique label is detected\n",
    "            if len(np.unique(cluster_labels)) <= 1:\n",
    "                continue\n",
    "\n",
    "            # Calculate silhouette score\n",
    "            silhouette_avg = silhouette_score(example_data, cluster_labels)\n",
    "\n",
    "            # Store silhouette score and parameters\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "            parameters.append({'EPS': eps, 'Min Samples': min_samples})\n",
    "\n",
    "            # Check if this silhouette score is better than the current best\n",
    "            if silhouette_avg > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette_avg\n",
    "                best_params = {'EPS': eps, 'Min Samples': min_samples}\n",
    "                best_cluster_labels = cluster_labels\n",
    "\n",
    "            # Plot clusters\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            num_features = example_data.shape[1]\n",
    "            print(f'DBSCAN Clustering - EPS: {eps}, Min Samples: {min_samples}')\n",
    "            fig, axs = plt.subplots(num_features, num_features, figsize=(15, 15))\n",
    "            for i in range(num_features):\n",
    "                for j in range(num_features):\n",
    "                    if i != j:\n",
    "                        axs[i, j].scatter(example_data.iloc[:, i], example_data.iloc[:, j], c=cluster_labels, cmap='rainbow', marker='o', edgecolors='k')\n",
    "                        axs[i, j].set_xlabel(example_data.columns[i])\n",
    "                        axs[i, j].set_ylabel(example_data.columns[j])\n",
    "                    else:\n",
    "                        axs[i, j].axis('off')  # Turn off plot for same feature pairs\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "    # Plot silhouette score versus parameter variation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(silhouette_scores)), silhouette_scores, marker='o', linestyle='-')\n",
    "    plt.title('Silhouette Score versus Parameter Variation')\n",
    "    plt.xlabel('Parameter Combination')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.xticks(range(len(silhouette_scores)), parameters, rotation=90)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the best silhouette score and its corresponding parameters\n",
    "    print(\"Best Silhouette Score:\", best_silhouette_score)\n",
    "    print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The silhouette score of K_means is higher than Hierarchal and DSCAN which means that in this dataset the K-means is better in clustering\n",
    "#of the iris dataset with K=2 and that silhoutte of DBSAN and Hirroircal are very close to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def GMMClustering_Irissssss(df):\n",
    "    # Define covariance types to test\n",
    "    covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
    "\n",
    "    # Initialize variables to store the best silhouette score and its corresponding parameters\n",
    "    best_silhouette_score = -1\n",
    "    best_params = {}\n",
    "\n",
    "    # Initialize lists to store silhouette scores and corresponding parameters\n",
    "    silhouette_scores = []\n",
    "    parameters = []\n",
    "\n",
    "    # Iterate over covariance types\n",
    "    for covariance_type in covariance_types:\n",
    "        # Perform clustering\n",
    "        gmm = GaussianMixture(n_components=3, covariance_type=covariance_type)\n",
    "        cluster_labels = gmm.fit_predict(df)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(df, cluster_labels)\n",
    "\n",
    "        # Store silhouette score and parameters\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        parameters.append({'Covariance Type': covariance_type})\n",
    "\n",
    "        # Check if this silhouette score is better than the current best\n",
    "        if silhouette_avg > best_silhouette_score:\n",
    "            best_silhouette_score = silhouette_avg\n",
    "            best_params = {'Covariance Type': covariance_type}\n",
    "            best_cluster_labels = cluster_labels\n",
    "\n",
    "        # Plot scatter plot for each pair of features\n",
    "        print(f'GMM Clustering - Covariance Type: {covariance_type}')\n",
    "        n_features = df.shape[1]\n",
    "        for i in range(n_features):\n",
    "            for j in range(i + 1, n_features):\n",
    "                plt.figure(figsize=(10, 6))\n",
    "\n",
    "                # Scatter plot\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.scatter(df.iloc[:, i], df.iloc[:, j], c=best_cluster_labels, cmap='viridis', s=50, alpha=0.5)\n",
    "                plt.xlabel(f'Feature {i}')\n",
    "                plt.ylabel(f'Feature {j}')\n",
    "                plt.title('Scatter Plot')\n",
    "\n",
    "                # Fit GaussianMixture model for the current pair of features\n",
    "                gmm_pair = GaussianMixture(n_components=3, covariance_type=best_params['Covariance Type'])\n",
    "                X_pair = df[[df.columns[i], df.columns[j]]]\n",
    "                gmm_pair.fit(X_pair)\n",
    "\n",
    "                # Contour plot for Gaussian mixture\n",
    "                plt.subplot(1, 2, 2)\n",
    "                x_min, x_max = df.iloc[:, i].min() - 1, df.iloc[:, i].max() + 1\n",
    "                y_min, y_max = df.iloc[:, j].min() - 1, df.iloc[:, j].max() + 1\n",
    "                xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                                     np.linspace(y_min, y_max, 100))\n",
    "                Z = gmm_pair.score_samples(np.column_stack([xx.ravel(), yy.ravel()]))\n",
    "                Z = Z.reshape(xx.shape)\n",
    "                plt.contourf(xx, yy, Z, cmap='viridis', levels=20, alpha=0.5)\n",
    "                plt.xlabel(f'Feature {i}')\n",
    "                plt.ylabel(f'Feature {j}')\n",
    "                plt.title('Contour Plot')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "    # Plot silhouette score versus covariance type\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(silhouette_scores)), silhouette_scores, tick_label=[param['Covariance Type'] for param in parameters])\n",
    "    plt.title('Silhouette Score versus Covariance Type')\n",
    "    plt.xlabel('Covariance Type')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Print the best silhouette score and its corresponding parameters\n",
    "    print(\"Best Silhouette Score:\", best_silhouette_score)\n",
    "    print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a high silhouette score generally indicates good clustering, as it implies dense, well-separated clusters with minimal overlap\n",
    "#the silueheette of GMM is very small compared to other clustering techniques in the data which results that K-means is the best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize_the Iris_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the DataFrame using MinMaxScaler\n",
    "normalized_data = pd.DataFrame(min_max_scaler.fit_transform(iris_df), columns=iris_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most of the Silhouette Score are very close to each as most of them are 0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2oBmWT2udcA"
   },
   "source": [
    "## Customer dataset\n",
    "Repeat all the above on the customer data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Customer_df=pd.read_csv('F:/Machine_Learning/Customer_data.csv')\n",
    "Customer_df.set_index('ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this the Hierorical technquie is the best as 0.75 however other are too small and DSBSCAN couldn't work without any Normalise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize_Customer_df\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "Normalize_Customer_df=Customer_df.copy()\n",
    "\n",
    "# Extract numerical columns\n",
    "numerical_columns = Normalize_Customer_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Create a MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max Scaling to the numerical columns\n",
    "Normalize_Customer_df[numerical_columns] = scaler.fit_transform(Normalize_Customer_df[numerical_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Form the precious the best silheutte score is from DBSCAN that is equal to 0.5209 and other are too small compared to it which means that this technquie is the best in the clustering "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Clustering Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
